# Iseer Architecture

**A Novel Mamba Ã— MoE Hybrid Language Model**

Built from scratch by [Inan](https://github.com/InanXR) â€¢ Iseer & Co.

---

## ğŸ§¬ Architecture

Iseer combines two cutting-edge innovations:
- **Mamba (State Space Models)** â€” O(n) linear sequence modeling
- **Mixture of Experts (MoE)** â€” Sparse activation for efficiency

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ISEER BLOCK                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input â†’ RMSNorm â†’ Mamba SSM            â”‚
â”‚            â†“                            â”‚
â”‚  + residual                             â”‚
â”‚            â†“                            â”‚
â”‚  RMSNorm â†’ MoE (top-k routing)          â”‚
â”‚            â†“                            â”‚
â”‚  + residual â†’ Output                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

- **Linear Complexity**: O(n) instead of O(nÂ²) attention
- **Sparse Compute**: Only top-k experts active per token
- **Triton Kernels**: Custom GPU kernels for selective scan
- **Bilingual**: Trained on Bengali + English

## ğŸ“¦ Installation

```bash
pip install -e .
```

## ğŸ”§ Usage

```python
from iseer import Iseer, IseerConfig

config = IseerConfig(
    vocab_size=32000,
    d_model=512,
    n_layers=12,
    n_experts=8,
    top_k=2,
)

model = Iseer(config)
```

## ğŸ“Š Model Variants

| Model | Params | Active | Context |
|-------|--------|--------|---------|
| ISEER-SM | 30M | 20M | 2048 |
| ISEER-MD | 120M | 45M | 4096 |

## ğŸ“„ License

MIT

## ğŸ”— Links

- [Iseer & Co.](https://iseer.co)
- [Paper (coming soon)]()
